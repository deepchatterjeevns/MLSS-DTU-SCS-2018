{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch basics.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "5wjF_2TXWmQX",
        "orTHDcQ6XvkB",
        "LVyEKCDCXqcz",
        "uCETqM3iXyPs",
        "K1n8SGpq_XmC",
        "qH05WCOTX8kX",
        "rKepCOedX9zx"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "NkyC0kiBUMLZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Install PyTorch\n",
        "\n",
        "Colab notebooks come with many packages, but PyTorch isn't one of them. Fortunately we can execute shell commands, and so it can be easily installed using pip."
      ]
    },
    {
      "metadata": {
        "id": "JH-AEwS7UEuq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kOMPHpONUcKd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Verify installation and runtime\n",
        "Google is kind enough to provide all of us with some free GPU compute time."
      ]
    },
    {
      "metadata": {
        "id": "ljrgSD3wUIxj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "assert torch.__version__ == '0.4.1'\n",
        "assert torch.cuda.device_count() > 0, 'Runtime > Change runtime type > Hardware accelerator > Choose GPU'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i452HbseU_LP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Simple operations\n",
        "\n",
        "This section will cover the basics of working with PyTorch tensors, including:\n",
        "- creating tensors and inspecting their properties\n",
        "- various ways of performing mathematical operations on them\n",
        "- instructions for using the documentation\n",
        "- NumPy integration\n",
        "- broadcasting\n",
        "- representation\n",
        "- GPU usage\n",
        "- serialization"
      ]
    },
    {
      "metadata": {
        "id": "5wjF_2TXWmQX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Factory functions\n",
        "\n",
        "There are many functions you can use to create new tensors. Some of the most useful include:\n",
        "- `torch.asarray` - like `np.asarray`\n",
        "- `torch.ones` - one-filled \n",
        "- `torch.zeros` - zero-filled \n",
        "- `torch.full` - constant-filled \n",
        "- `torch.randn` - random normal numbers\n",
        "- `torch.empty` - uninitialized values (for fast creation of temporaries)\n",
        "\n",
        "All of those functions take in a shape argument, an optional `dtype` specifying the data type of individual elements (defaults to `float32`), and a device where the tensor should be allocated (e.g. can be a GPU, defaults to `'cpu'`).\n"
      ]
    },
    {
      "metadata": {
        "id": "79_YoYcQVi9H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = torch.as_tensor([2., 3., 4., 5.])\n",
        "y = torch.full_like(x, 5, dtype=torch.int64)\n",
        "print(x, x.shape, x.dtype)\n",
        "print(y, y.shape, y.dtype)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "orTHDcQ6XvkB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Simple operations\n",
        "\n",
        "Tensors support all Python operators, and over 200 specialized mathematical operations. All of them are available both as methods and as functions in the torch package."
      ]
    },
    {
      "metadata": {
        "id": "_NB5r72jX_cY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "z = torch.arange(4, dtype=torch.float)\n",
        "print(x + z, x * z, x.dot(z), x.lgamma(), x > z)\n",
        "print(torch.add(x, z), torch.mul(x, z), torch.dot(x, z), torch.lgamma(x), torch.gt(x, z))\n",
        "print('Tensor functions:')\n",
        "print('\\n'.join(n for n in dir(x) if not n.startswith('_') and not n.endswith('_')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "atJtCxXIkDXd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Resizing: If you want to resize/reshape tensor, you can use ``torch.reshape``:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "2n9jaWt5kDXe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "q = torch.randn(4, 4)\n",
        "w = q.reshape(16)\n",
        "r = q.reshape(-1, 8)  # the size -1 is inferred from other dimensions\n",
        "p = q[None, :, 0]  # all of NumPy indexing is also supported, None creates a new dimension of size 1\n",
        "print(q.shape, w.shape, r.shape, p.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y8W4356XkDXh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you have a one element tensor, use ``.item()`` to get the value as a\n",
        "Python number\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Cswa6DmYkDXh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "s = torch.randn(1)\n",
        "print(s)\n",
        "print(s.item())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LVyEKCDCXqcz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## In-place modifications\n",
        "\n",
        "Expressions like `x + z` always allocate a new tensor for the result, to avoid modifying the arguments. However, sometimes it is more convenient to modify a tensor value in-place without allocating any intermediates.\n",
        "\n",
        "> NOTE: operations that mutate tensors given as their arguments always have names ending with an underscore."
      ]
    },
    {
      "metadata": {
        "id": "pvNQZiymXros",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "xc = x.clone() # .clone() will ensure that modifying xc won't affect the value of x\n",
        "print(xc)\n",
        "xc.add_(2)\n",
        "print(xc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uCETqM3iXyPs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Documentation pointers\n",
        "\n",
        "Complete documentation listing all available operations can be found at [the PyTorch website](https://pytorch.org/docs).\n",
        "\n",
        "#### Exercise\n",
        "\n",
        "`torch.permute` is an operation that lets you rearrange the order tensor dimensions in an arbitrary way. Find it's documentation and fill in the cell below to satisfy the assert."
      ]
    },
    {
      "metadata": {
        "id": "cCtM3XGQaFFN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "t = torch.randn((2, 3, 4, 5, 6, 7))\n",
        "assert t.permute(...).shape == (4, 3, 7, 2, 6, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c_3BugKmX11v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Broadcasting\n",
        "\n",
        "PyTorch tensors support broadcasting, according to NumPy rules.\n",
        "\n",
        "![broadcasting](https://jakevdp.github.io/PythonDataScienceHandbook/figures/02.05-broadcasting.png =500x)\n",
        "\n",
        ">> Image taken from [here](https://jakevdp.github.io/PythonDataScienceHandbook/02.05-computation-on-arrays-broadcasting.html).\n",
        "\n",
        "#### Exercise\n",
        "\n",
        "Read [the NumPy guide for broadcasting](https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html#general-broadcasting-rules), work out the shape of `c * d` and fill in the assert below to make it pass."
      ]
    },
    {
      "metadata": {
        "id": "GGRoCiBrcbdW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "a = torch.randn(2, 1, 4)\n",
        "b = torch.randn(7, 2, 3, 1)\n",
        "# NOTE: we align the dimensions to the right to figure out the result shape!\n",
        "#      a:    2, 1, 4\n",
        "#      b: 7, 2, 3, 1\n",
        "# result: 7, 2, 3, 4\n",
        "assert a.add(b).shape == (7, 2, 3, 4)\n",
        "\n",
        "c = torch.randn(8, 1, 1, 4, 5, 1)\n",
        "d = torch.randn(6, 1, 4, 1, 1)\n",
        "assert c.add(d).shape == (...)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K1n8SGpq_XmC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## NumPy interoperability\n",
        "\n",
        "PyTorch tensors can be casted into NumPy arrays (and the other way around) with almost no cost. The relevant functions are `.numpy()` and `torch.from_numpy` (or `torch.as_tensor`)."
      ]
    },
    {
      "metadata": {
        "id": "9oiT_pGO_bio",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = torch.randn(4)\n",
        "print(x)\n",
        "print(x.numpy(), type(x.numpy()))\n",
        "print(torch.from_numpy(x.numpy()))\n",
        "print(torch.as_tensor(x.numpy()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qH05WCOTX8kX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Using GPUs\n",
        "\n",
        "To use a GPU simply call `.cuda()`, `.to('cuda')`, or specify `device='cuda'` when creating a tensor. GPU tensors support exactly the same set of operations as CPU tensors, but they are much faster for larger arrays.\n",
        "\n",
        "> NOTE: While GPUs can be an order of magnitude faster than CPUs, they really start to shine only once the workload is big enough. Don't be surprised if toy benchmarks show that they are slower for tiny arrays."
      ]
    },
    {
      "metadata": {
        "id": "ViptdZxfX2qI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "xc = x.cuda()\n",
        "mc = torch.ones(4, 4, device='cuda')\n",
        "print(x.device, xc.device)\n",
        "print(xc)\n",
        "result = torch.matmul(mc, xc) + 2\n",
        "print(result)\n",
        "print(result.cpu())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rKepCOedX9zx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Serialization\n",
        "\n",
        "PyTorch provides its own serialization methods, mimicking the standard `pickle` module. It's not recommended to use `pickle` with PyTorch tensors, because `torch.*` functions are more efficient and make it possible to e.g. load GPU tensors onto the CPU (whereas `pickle` would fail)."
      ]
    },
    {
      "metadata": {
        "id": "a0QidF4FX-qs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.save(x, 'x.pt')\n",
        "assert (torch.load('x.pt') == x).all()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7EropxLBgJx8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Automatic differentiation\n",
        "\n",
        "`torch.autograd` enables efficient differentiation of any functions operating on PyTorch tensors.\n",
        "\n",
        "AD significantly increases memory usage, and because of that it is disabled by default. To enable it, specify `requires_grad=True` when creating tensors w.r.t. which you'll want to differentiate. Alternatively, if the formula for tensor instantiation is more complicated, use the `.requires_grad_()` method to set it.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "2bfNPs9ye0CU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = torch.arange(4, dtype=torch.float, requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "y = torch.randn(4).requires_grad_(True)\n",
        "assert y.requires_grad\n",
        "y.requires_grad_(False)\n",
        "assert not y.requires_grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P9Zi-wLViUVj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Any result of an operation which had at least a single argument whith `requires_grad` will require it as well (you can think of it as of a viral flag)."
      ]
    },
    {
      "metadata": {
        "id": "l19s4u5uhosJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "z = x * y\n",
        "print(z.requires_grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N7vBB4a7jItp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can differentiate through more complicated functions, but remember that PyTorch can only compute gradients (Jacobians of scalar-valued functions), or Jacobian-vector products. It is possible to recover the full Jacobian, but you'll need to differentiate the function as many times as the number of its outputs."
      ]
    },
    {
      "metadata": {
        "id": "xyZQvRz0iiay",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def poly(x):\n",
        "  return x ** 2 + 5 * x + 2\n",
        "\n",
        "def d_poly(x):\n",
        "  return 2 * x + 5\n",
        "\n",
        "y = poly(x)\n",
        "assert y.requires_grad\n",
        "# torch.matmul(jacobian(poly), ones) == d_poly, because jacobian(poly) is a diagonal matrix\n",
        "d_x, = torch.autograd.grad(y, x, torch.ones_like(y)) # d_x == d_y / d_x\n",
        "assert (d_x == d_poly(x)).all()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6BHu6oHwmIX2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A different way to obtain gradients is to use the `.backward()` method. This is the most common method and you'll find it in pretty much every single PyTorch script. Unlike `torch.autograd.grad`, this call has a side effect of accumulating the computed gradients in the `.grad` attributes of all inputs which required gradients."
      ]
    },
    {
      "metadata": {
        "id": "UaRHOcsVlh0h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = torch.arange(4, dtype=torch.float, requires_grad=True)\n",
        "# NOTE: We need .sum() to get a scalar that we can differentiate.\n",
        "#       Since d x.sum() / d x == torch.ones_like(x), we get behavior equivalent to one above.\n",
        "#       In any real-world examples this will be a scalar loss you'll be optimizing, so this won't\n",
        "#       be necessary.\n",
        "poly(x).sum().backward()\n",
        "print(x.grad)\n",
        "poly(x).sum().backward()\n",
        "print(x.grad) # NOTE: this is 2x the initial value, because the gradients are accumulated."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JRNfSXbPnJX8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Locally disabling differentiation\n",
        "\n",
        "Sometimes you might want to disable AD (e.g. to evaluate your model without optimizing it), and it would be very inconvenient to change `.requires_grad` of every single input or model parameter. Fortunately, there's `torch.no_grad()`, which is a context manager that completely turns of the AD logic, irrespectively of any flags."
      ]
    },
    {
      "metadata": {
        "id": "wPwEHrPtmmSp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = torch.randn(4, requires_grad=True)\n",
        "assert (x * 2).requires_grad\n",
        "with torch.no_grad():\n",
        "  assert x.requires_grad            # no-grad mode doesn't affect existing tensors,\n",
        "  assert not (x * 2).requires_grad  # but any newly created ones won't affect AD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TD8p-LY1n2B7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Differentiating through control flow\n",
        "\n",
        "One of the strenghts of `torch.autograd` is that it's completely unaffected by the actual code you write. Loops, conditionals, generators, coroutines - any Python construct can be differentiated successfully."
      ]
    },
    {
      "metadata": {
        "id": "kHfTtSmanjDo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = torch.randn(4, requires_grad=True)\n",
        "y = 1 + torch.rand(()) # y will be a scalar value sampled uniformly from [1, 2]\n",
        "print(y)\n",
        "z = x\n",
        "while z.norm() < 10:\n",
        "  z = z * y\n",
        "z.sum().backward()\n",
        "print(x.grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U_78-dApoKdJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Fit a linear model\n",
        "\n",
        "Material which we've covered above is sufficient to start coding simple models. In this exercise you'll create a train a linear regression model. \n",
        "\n",
        "#### Model\n",
        "\n",
        "Since both input and output have one dimension, our model can be expressed as\n",
        "\n",
        "$$w \\cdot x + b$$\n",
        "\n",
        "where $w$ and $b$ are 1 dimensional tensors representing the weight and bias. Note that $x$ can have arbitrarily many elements (as long as it's 1D), and broadcasting will ensure that this expression is still correct (and will compute an output for every input element in parallel).\n",
        "\n",
        "#### Loss\n",
        "\n",
        "The loss function we'll use is called mean squared error and can be computed as:\n",
        "\n",
        "$$\\frac{1}{N}(\\mathrm{output} - \\mathrm{target})^2$$\n",
        "\n",
        "where $N$ is the number of examples, $\\mathrm{output}$ is the model output, and $\\mathrm{target}$ is the expected output value.\n",
        "\n",
        "#### Gradient descent\n",
        "\n",
        "Our initial guess for the weight and bias will be probably far off, but we can use gradient descent to optimize those values. This amounts to taking the derivatives of the loss with respect to both the weight and bias, and performing an update, given as:\n",
        "\n",
        "$$\\theta' = \\theta - \\eta \\frac{\\delta L}{\\delta \\theta}$$\n",
        "\n",
        "where $\\eta$ is the learning rate, and $\\frac{\\delta L}{\\delta \\theta}$ is a gradient of the loss value."
      ]
    },
    {
      "metadata": {
        "id": "gAl5cqTVoREk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_samples = 100\n",
        "x = torch.linspace(-5, 5, steps=num_samples, dtype=torch.float)\n",
        "y = x * 0.6 + torch.randn(num_samples) * 1.5\n",
        "\n",
        "def model(x, W, b):\n",
        "  return ... # Write out a formula for a linear model\n",
        "\n",
        "lr = 1e-3\n",
        "W = ... # Create a randomly initialized 1-element weight tensor\n",
        "b = ... # Create a randomly initialized 1-element bias tensor\n",
        "\n",
        "for i in range(20):\n",
        "  pred = model(x, W, b)\n",
        "  loss = ... # Write a formula for the MSE loss\n",
        "  grad_W, grad_b = torch.autograd.grad(loss, (W, b))\n",
        "  with torch.no_grad():\n",
        "    W -= ... # Write out an gradient descent update for W\n",
        "    ... # Write out an gradient descent update for b\n",
        "  print(loss.item())\n",
        "\n",
        "# Plot the data and the fitted model\n",
        "plt.scatter(x, y)\n",
        "plt.plot(x.numpy(), model(x, W, b).detach().numpy(), 'r')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y4Mzg7OBoSYl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Rewrite the code above using torch.nn\n",
        "\n",
        "Our model can be easily represented by an `nn.Linear` layer from `torch.nn`. Let's rewrite the exercise from above using more standard methods.\n",
        "\n",
        "> Do you know why did we have to reshape the dataset to be of size `(num_samples, 1)`? If not, check `nn.Linear` docs for shapes it expects."
      ]
    },
    {
      "metadata": {
        "id": "u8F3xnoBoXaJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_samples = 100\n",
        "x = torch.linspace(-5, 5, steps=num_samples, dtype=torch.float).unsqueeze(1)\n",
        "y = x * 0.6 + torch.randn(num_samples, 1) * 1.5\n",
        "\n",
        "model = ...             # Use the docs for torch.nn to figure out how to create this model\n",
        "optimizer = ...         # Use the docs for torch.optim package to create an SGD optimizer\n",
        "\n",
        "for i in range(20):\n",
        "  pred = model(x)\n",
        "  loss = ...            # Use the MSE function from PyTorch\n",
        "  optimizer.zero_grad() # Zero the .grad parameters (remember they accumulate!)\n",
        "  loss.backward()       # Compute the gradients\n",
        "  optimizer.step()      # Update parameters\n",
        "  print(loss.item())\n",
        "\n",
        "# Plot the data and the fitted model\n",
        "plt.scatter(x, y)\n",
        "plt.plot(x.numpy(), model(x).detach().numpy(), 'r')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IhYPSFrQwWO7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 3: Make the above model train on the GPU\n",
        "\n",
        "Copy the code from above and modify it to train the model on the GPU. Of course it's an overkill to use it for models this small, but it will be important when we get to more complicated networks."
      ]
    },
    {
      "metadata": {
        "id": "oWDgvzMOvlg0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Copy the code from above and modify it to train the model on the GPU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "83IQ-jseCENT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 4: Fit a nonlinear model to a more complicated dataset\n",
        "\n",
        "Linear models were enough to fit previous datasets. Can you build up a more complicated non-linear model using `torch.nn`? An example can be found e.g. [here](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/feedforward_neural_network/main.py#L38-L49)."
      ]
    },
    {
      "metadata": {
        "id": "isKsRsJdCJeU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_samples = 100\n",
        "x = torch.linspace(-1, 4, steps=num_samples, dtype=torch.float).unsqueeze(1)\n",
        "y = 0.5 * x ** 4 + -2.2 * x ** 3 + 1.22 * x ** 2 + -0.05 * x + torch.randn(num_samples, 1) * 1.5\n",
        "\n",
        "model = ...             # Use the docs for torch.nn to figure out how to create this model\n",
        "optimizer = ...         # Use the docs for torch.optim package to create an SGD optimizer\n",
        "\n",
        "for i in range(2000):\n",
        "  pred = model(x)\n",
        "  loss = ...            # Use the MSE function from PyTorch\n",
        "  optimizer.zero_grad() # Zero the .grad parameters (remember they accumulate!)\n",
        "  loss.backward()       # Compute the gradients\n",
        "  optimizer.step()      # Update parameters\n",
        "  if i % 100 == 0:\n",
        "    print(loss.item())\n",
        "\n",
        "# Plot the data and the fitted model\n",
        "plt.scatter(x, y)\n",
        "plt.plot(x.numpy(), model(x).detach().numpy(), 'r')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}